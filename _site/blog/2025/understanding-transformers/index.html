<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Understanding the Attention Mechanism in Transformers | Piyush Bhardwaj
    
  
</title>
<meta name="author" content="Piyush Bhardwaj">
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/al-folio/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">



<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/al-folio/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" href="/al-folio/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/al-folio/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->




  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/al-folio/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://0.0.0.0:8080/al-folio/blog/2025/understanding-transformers/">


  <!-- Dark Mode -->
  <script src="/al-folio/assets/js/theme.js?54695281cc2353e1320ffa8e9a185978"></script>
  <link defer rel="stylesheet" href="/al-folio/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>









  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/al-folio/">
          
            
              <span class="font-weight-bold">Piyush</span>
            
            
            Bhardwaj
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/al-folio/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item active">
                  <a class="nav-link" href="/al-folio/blog/">blog
                    
                  </a>
                </li>
              
            
          
          
            <!-- Search -->
            <li class="nav-item">
              <button id="search-toggle" title="Search" onclick="openSearchModal()">
                <span class="nav-link">ctrl k <i class="ti ti-search"></i></span>
              </button>
            </li>
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        






<div class="post">
  <header class="post-header">
    <h1 class="post-title">Understanding the Attention Mechanism in Transformers</h1>
    <p class="post-meta">
      Created on October 28, 2025
      
      
      
    </p>
    <p class="post-tags">
      
        <a href="/al-folio/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>
      
      
          ·  
        
          
            <a href="/al-folio/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a>
          
          
             
          
        
          
            <a href="/al-folio/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers</a>
          
          
             
          
        
          
            <a href="/al-folio/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> neural-networks</a>
          
          
        
      

      
          ·  
        
          
            <a href="/al-folio/blog/category/deep-learning"> <i class="fa-solid fa-tag fa-sm"></i> deep-learning</a>
          
          
             
          
        
          
            <a href="/al-folio/blog/category/transformers"> <i class="fa-solid fa-tag fa-sm"></i> transformers</a>
          
          
        
      
    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <p>At an API level, an <strong>attention layer</strong> is a sequence to sequence model that takes a sequence of input vectors and produces a new, modified sequence of output vectors of the same or different dimension.</p>

<p>This note builds up the attention mechanism step by step, starting from the simplest case and gradually adding complexity. We will proceed as follows:</p>

<ol>
  <li>Attention with a single query vector</li>
  <li>Attention with a sequence of query vectors</li>
  <li>Attention with learnable parameters</li>
</ol>

<p>An attention layer receives a sequence of <strong>query</strong> vectors, a sequence of <strong>key</strong> vectors and a corresponding set of <strong>value</strong> vectors (same number as keys). Attention layer behaves like a fuzzy dictionary, finding keys closest to the query and returning the value corresponding to the keys. The final returned value is weighted sum of all given values with weights being equal to the similarity between the corresponding key and query. We will show mathematically how this is done.</p>

<h4 id="1-attention-with-a-single-query-vector">1. Attention with a Single Query Vector</h4>
<p>In the first setting, we consider a case where the sequence of query vectors has only a single query vector. This is represented by matrix $Q_{1\times d}$. We have $N_k$ keys represented by rows of matrix $K_{N_k \times d}$. Number of value vectors are equal to number of keys $N_k$, as corresponding to each key there is a single value.</p>

<p>Similarity matrix is computed as:</p>

\[S_{1\times N_k} = Softmax(Q_{1\times d}K_{N_k\times d}^T)\]

<p>Finally, to get the output value vector we will take a weighted sum of $N_k$ “value” vectors, where the weights are based on the similarity scores in S. The scores are normalized by applying softmax operation across each <strong>row</strong> of matrix $QK^T$. The softmax ensures that all weights are positive and sum to one. Note that we need each query and key to be in the same dimension space to allow for dot product similarity.</p>

\[Attention(Q,K,V)_{1\times d} = S_{1\times N_k}V_{N_k\times d}\]

<h6 id="computational-complexity">Computational Complexity</h6>

<ol>
  <li>Computation of matrix $QK^T$ takes $N_k\times d$ multiplication operations</li>
  <li>Normalization through softmax takes $N_k$ steps, considdering $exp()$ operations takes$O(1)$ time with small constant as it is hardware accelerated on GPU and on CPU fast exponentiation libraries like in math.h in C++ exist.</li>
  <li>Finally computing weighted sum of “values” will take $N_k\times d$ multiplications.</li>
</ol>

\[{\text{Time complexity: } O(N_k d)}\]

<h4 id="2-attention-with-a-sequence-of-query-vectors">2. Attention with a Sequence of Query Vectors</h4>
<p>Now lets consider slightly more complicated setting where we have $N_q$ queries, each represented by a row in matrix $Q_{N_q \times d}$. Similar to single query scenario, we compute dot product similarity between each query and key as</p>

\[S_{N_q \times N_k} = softmax(QK^T)\]

<p>Each row of S represents similarity score of that query with all the $N_k$ keys. Finally multiply S with value matrix $V_{N_k \times d}$ to get $N_q$ output values.</p>

\[Attention(Q,K,V)_{N_q\times d} = S_{N_q \times N_k} V_{N_k \times d}\]

\[\text{Time complexity}: O(N_q \times N_k \times d)\]

<p>This is assuming that “query” vector dimension and “value” vector dimension are the same. However, this may not always be true. If “value” vector has dimension d_v:</p>

\[\text{Time complexity}:O(N_q \times N_k \times d + N_q \times N_k \times d_v)\]

<p>In <strong>self-attention</strong>, same sequence of 
vectors are passed as key, query and values i.e. $N_q = N_v = N_k = N$ and we assume output value vectors to be in same dimension i.e. $d_v = d$. Thus, we get</p>

\[\boxed{\text{Time complexity}: O(N^2*d)}\]

<h5 id="scaling-softmax">Scaling Softmax</h5>
<p>There is also a scaling nuance we need to address before moving further. Let’s focus on similarity matrix $S$. Suppose each entry of query Q and key K is an independent random variable with $\mu=0$, $\sigma^2 = 1$. Then,</p>

\[\begin{align*}E[s_{ij}] &amp;= E[\mathbf{q}_i \cdot \mathbf{k}_j] = E\left[\sum_t q_{it}k_{jt}\right] = \sum_t E[q_{it}k_{jt}] \text{ (by linearity of expectation)} \\
&amp;= \sum_t E[q_{it}]E[k_{jt}] \text{ (independence of random variables) } \\
&amp;= 0
\end{align*}\]

<p>So, expected value of each term before softmax operation is 0. Let’s also check its variance.</p>

\[\begin{align*}
\operatorname{Var}[s_{ij}] &amp;= \operatorname{Var}\left[\sum_t q_{it} k_{jt}\right] \\
&amp;= \sum_t \operatorname{Var}[q_{it} k_{jt}] \text{ (since each of summand is independent)}\\
&amp;= \sum \operatorname{Var}[q_{it}] \operatorname{Var}[k_{jt}] \\
&amp;= d
\end{align*}\]

<p>Last equality follows since $Var(XY) = E[X^2Y^2]- (E[XY])^2 = E[X^2]E[Y^2] = Var(X)Var(Y)$</p>

<p>This shows that each $s_{ij}$ has mean $0$ but variance is large ($d$). As we do softmax, for large values of $s_{ij}$, softmax saturates i.e. the derivative of softmax vanishes leading to learning being stopped in gradient descent. To avoid this we need to prevent $s_{ij}$ from getting too large. We scale $s_{ij}$ by $\frac{1}{\sqrt d}$ as $var[s_{ij}/\sqrt(d)] = var[s_{ij}]/d = 1$</p>

<h4 id="3-attention-with-learnable-weights-parameters">3. Attention with learnable weights parameters</h4>
<p>So far the attention block has no learnable parameters, it is simply a fuzzy dictionary. As next layer of complexity we linearly transform the queries, keys and values into another vector space of different dimension before computing similarity matrix.</p>
<ol>
  <li>Keys are linearly transformed as</li>
</ol>

\[\hat{K}_{N_k \times d_k} = K_{N_k \times d}*W^K_{d \times d_k}\]

<ol>
  <li>Queries are in the same dimension space as keys</li>
</ol>

\[\hat{Q}_{N_q \times d_k} = Q_{N_q \times d}*W^Q_{d \times d_k}\]

<ol>
  <li>Values are transformed as</li>
</ol>

\[\hat{V}_{N_k X d_v} = V_{N_k \times d}*W^V_{d \times d_v}\]

<p>Again we do the same similarity matrix computation followed by softmax and multiplication by V_hat to get attention output sequence.</p>

\[\boxed{Attention(Q,K,V)_{N_q\times d} = softmax(\frac{\hat{Q}\hat{K}^T}{\sqrt{d}})\hat{V}}\]

<p>Time Complexity is $O(N_k \times N_q \times d_k + N_k \times N_q \times d_v + N_k \times d_k \times d + N_q \times d_k \times d + N_k \times d_v \times d)$.</p>

<p>For self attention, assuming output sequence vector is of dimension $d_v = d$:</p>

\[O(N^2d + Nd^2 )\]

<p>since $N » d$ (long context window) self attention is practically $O(N^2d)$. The qudratic complexity in context length makes attention expensive for long sequences.</p>

<p>##### How many learnable parameters does attention layer has? 
   Considering keys, queries and values are all transformed to same dimension $d$ equal to the input dimension, total learnable parameters: $3d^2$. Note, learnable parameters matrix doesn’t depend on length of input sequence. This makes intuitive sense, as each training example can be of different sequence length (each input sentence can have different number of words/token) and still we can update parameters of the model.</p>

<hr>

<h2 id="closing-remarks">Closing Remarks</h2>

<p>Further extensions — <strong>multi-head attention</strong>, <strong>cross-attention</strong>, and <strong>masked attention</strong> — expand this mechanism to capture richer relationships. We will explore these in future notes.</p>

    </div>
  </article>

  

  

  
    
      

  
    
    
      <br>
      <hr>
      <br>
      <ul class="list-disc pl-8"></ul>

      <!-- Adds related posts to the end of an article -->
      <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    

    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    
      <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/al-folio/blog/2015/math/">a post with math</a>
    
  </li>



    
  

  
  
</div>

      
    </div>

    <!-- Footer -->
    


  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      
  © Copyright 2025
  Piyush
  
  Bhardwaj. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. #Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

  
  

    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="/al-folio/assets/js/bootstrap.bundle.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>


  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/al-folio/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script>























  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/al-folio/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>






<!-- Load Common JS -->
<script src="/al-folio/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/al-folio/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/al-folio/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/al-folio/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>

<!-- Badges -->

  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>



  <!-- MathJax -->
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
  
    <script src="/al-folio/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  









  <!-- Scrolling Progress Bar -->
  <script defer src="/al-folio/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script>







  <!-- Back to Top -->
  <script src="/al-folio/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>



  <!-- Search -->
  <script type="module" src="/al-folio/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script>
  <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys>
  <script src="/al-folio/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script>
  <script src="/al-folio/assets/js/search-data.js"></script>
  <script src="/al-folio/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script>




  </body>
</html>
